import type { Metadata } from "next";
import { notFound } from "next/navigation";
import { LabPageClient } from "./LabPageClient";

// Lab information mapping - aligned with ThreatModelDiagram component
const labsData = {
    "prompt-injection": {
        id: "LLM01",
        title: "Prompt Injection",
        description: "Manipulating LLMs via crafted inputs can lead to unauthorized access, data breaches, and compromised decision-making. Learn hands-on prompt injection techniques and discover hidden flags.",
        keywords: ["prompt injection", "LLM01", "crafted inputs", "unauthorized access", "jailbreaking"],
        component: "PromptInjectionLab"
    },
    "sensitive-info-disclosure": {
        id: "LLM02",
        title: "Sensitive Information Disclosure",
        description: "Sensitive information can affect both the LLM and its application context. This includes personal identifiable information (PII), financial details, health records, confidential business data, security credentials, and legal documents.",
        keywords: ["sensitive information disclosure", "LLM02", "PII", "confidential data", "data extraction"],
        component: "SensitiveInfoLab"
    },
    "supply-chain": {
        id: "LLM03",
        title: "Supply Chain",
        description: "LLM supply chains are susceptible to various vulnerabilities, which can affect the integrity of training data, models, and deployment platforms. These risks can result in biased outputs, security breaches, or system failures.",
        keywords: ["supply chain", "LLM03", "training data integrity", "deployment platforms", "biased outputs"],
        component: "SupplyChainLab"
    },
    "data-poisoning": {
        id: "LLM04",
        title: "Data and Model Poisoning",
        description: "Data poisoning occurs when pre-training, fine-tuning, or embedding data is manipulated to introduce vulnerabilities, backdoors, or biases.",
        keywords: ["data poisoning", "LLM04", "pre-training", "fine-tuning", "backdoors", "biases"],
        component: "DataPoisoningLab"
    },
    "improper-output": {
        id: "LLM05",
        title: "Improper Output Handling",
        description: "Improper Output Handling refers specifically to insufficient validation, sanitization, and handling of the outputs generated by large language models before they are passed downstream to other components and systems.",
        keywords: ["improper output handling", "LLM05", "output validation", "sanitization", "downstream systems"],
        component: "ImproperOutputLab"
    },
    "excessive-agency": {
        id: "LLM06",
        title: "Excessive Agency",
        description: "Granting LLMs unchecked autonomy to take action can lead to unintended consequences, jeopardizing reliability, privacy, and trust.",
        keywords: ["excessive agency", "LLM06", "unchecked autonomy", "unintended consequences", "reliability"],
        component: "ExcessiveAgencyLab"
    },
    "system-prompt-leakage": {
        id: "LLM07",
        title: "System Prompt Leakage",
        description: "The system prompt leakage vulnerability in LLMs refers to the risk that the system prompts or instructions used to steer the behavior of the model can also contain sensitive information that was not intended to be discovered.",
        keywords: ["system prompt leakage", "LLM07", "system prompts", "model instructions", "sensitive information"],
        component: "SystemPromptLeakageLab"
    },
    "vector-embedding-weakness": {
        id: "LLM08",
        title: "Vector and Embedding Weaknesses",
        description: "Weaknesses in how vectors and embeddings are generated, stored, or retrieved can be exploited by malicious actions (intentional or unintentional) to inject harmful content, manipulate model outputs, or access sensitive information.",
        keywords: ["vector embedding weaknesses", "LLM08", "embedding vulnerabilities", "harmful content injection", "model output manipulation"],
        component: "VectorEmbeddingLab"
    },
    "misinformation": {
        id: "LLM09",
        title: "Misinformation",
        description: "Misinformation occurs when LLMs produce false or misleading information that appears credible.",
        keywords: ["misinformation", "LLM09", "false information", "misleading content", "credible appearance"],
        component: "MisinformationLab"
    },
    "unbounded-consumption": {
        id: "LLM10",
        title: "Unbounded Consumption",
        description: "Unbounded Consumption occurs when a Large Language Model (LLM) application allows users to conduct excessive and uncontrolled inferences, leading to risks such as denial of service (DoS), economic losses, model theft, and service degradation.",
        keywords: ["unbounded consumption", "LLM10", "excessive inferences", "denial of service", "economic losses", "model theft"],
        component: "UnboundedConsumptionLab"
    }
};

type Props = {
    params: Promise<{ labId: string }>
}

export async function generateMetadata({ params }: Props): Promise<Metadata> {
    const { labId } = await params;
    const lab = labsData[labId as keyof typeof labsData];

    if (!lab) {
        return {
            title: "Lab Not Found - LLM Security Labs",
            description: "The requested lab could not be found."
        };
    }

    return {
        title: `${lab.id}: ${lab.title} Lab - Interactive Security Training`,
        description: lab.description,
        keywords: [
            ...lab.keywords,
            "OWASP Top 10 LLM",
            "AI security",
            "cybersecurity training",
            "hands-on learning",
            "security vulnerability",
            "machine learning security"
        ],
        openGraph: {
            title: `${lab.id}: ${lab.title} Lab - Interactive Security Training`,
            description: lab.description,
            url: `https://www.llm-sec.dev/labs/${labId}`,
            images: [
                {
                    url: `/labs/${labId}-preview.png`,
                    width: 1200,
                    height: 630,
                    alt: `${lab.title} Lab Interface`
                }
            ]
        },
        twitter: {
            title: `${lab.id}: ${lab.title} Lab - Interactive Security Training`,
            description: lab.description,
            images: [`/labs/${labId}-preview.png`]
        },
        alternates: {
            canonical: `https://www.llm-sec.dev/labs/${labId}`
        }
    };
}

export default async function LabPage({ params }: Props) {
    const { labId } = await params;
    const lab = labsData[labId as keyof typeof labsData];

    if (!lab) {
        notFound();
    }

    const jsonLd = {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": `${lab.id}: ${lab.title} Lab`,
        "description": lab.description,
        "url": `https://www.llm-sec.dev/labs/${labId}`,
        "educationalLevel": "Intermediate",
        "teaches": lab.title,
        "learningResourceType": "Interactive Lab",
        "interactivityType": "active",
        "educationalUse": "instruction",
        "audience": {
            "@type": "EducationalAudience",
            "educationalRole": ["security professional", "developer", "researcher"]
        },
        "provider": {
            "@type": "Organization",
            "name": "LLM Security Labs"
        },
        "isPartOf": {
            "@type": "Course",
            "name": "OWASP Top 10 LLM Vulnerabilities",
            "provider": {
                "@type": "Organization",
                "name": "LLM Security Labs"
            }
        },
        "breadcrumb": {
            "@type": "BreadcrumbList",
            "itemListElement": [
                {
                    "@type": "ListItem",
                    "position": 1,
                    "name": "Home",
                    "item": "https://www.llm-sec.dev"
                },
                {
                    "@type": "ListItem",
                    "position": 2,
                    "name": "Labs",
                    "item": "https://www.llm-sec.dev/labs"
                },
                {
                    "@type": "ListItem",
                    "position": 3,
                    "name": lab.title,
                    "item": `https://www.llm-sec.dev/labs/${labId}`
                }
            ]
        }
    };

    return (
        <>
            <script
                type="application/ld+json"
                dangerouslySetInnerHTML={{ __html: JSON.stringify(jsonLd) }}
            />
            <LabPageClient labData={lab} />
        </>
    );
} 